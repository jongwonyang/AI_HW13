{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV0BaFtPtjJe"
      },
      "source": [
        "# Regularization and Gradient Descent Exercises\n",
        "![UnderOverFit.png](Assets/UnderOverFit.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-7jrpCJtjJn"
      },
      "source": [
        "# Learning Objectives\n",
        "\n",
        "- Explain cost functions, regularization, feature selection, and hyper-parameters\n",
        "- Summarize complex statistical optimization algorithms like gradient descent and its application to linear regression\n",
        "- Apply Intel速 Extension for Scikit-learn* to leverage underlying compute capabilities of hardware\n",
        "\n",
        "# scikit-learn* \n",
        "\n",
        "Frameworks provide structure that Data Scientists use to build code. Frameworks are more than just libraries, because in addition to callable code, frameworks influence how code is written. \n",
        "\n",
        "A main virtue of using an optimized framework is that code runs faster. Code that runs faster is just generally more convenient but when we begin looking at applied data science and AI models, we can see more material benefits. Here you will see how optimization, particularly hyperparameter optimization can benefit more than just speed. \n",
        "\n",
        "These exercises will demonstrate how to apply **the Intel速 Extension for Scikit-learn*,** a seamless way to speed up your Scikit-learn application. The acceleration is achieved through the use of the Intel速 oneAPI Data Analytics Library (oneDAL). Patching is the term used to extend scikit-learn with Intel optimizations and makes it a well-suited machine learning framework for dealing with real-life problems. \n",
        "\n",
        "To get optimized versions of many Scikit-learn algorithms using a patch() approach consisting of adding these lines of code after importing sklearn: \n",
        "\n",
        "- **from sklearnex import patch_sklearn**\n",
        "- **patch_sklearn()**\n",
        "\n",
        "## This exercise relies on installation of  Intel速 Extension for Scikit-learn*\n",
        "\n",
        "If you have not already done so, follow the instructions from Week 1 for instructions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aQKPPK2tjJo"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "We will begin with a short tutorial on regression, polynomial features, and regularization based on a very simple, sparse data set that contains a column of `x` data and associated `y` noisy data. The data file is called `X_Y_Sinusoid_Data.csv`. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os, sys\n",
        "drive.mount('gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6foRou8tkgB",
        "outputId": "0268dce2-c9d4-4993-a751-9aa1d8647a45"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:38:25.023951Z",
          "start_time": "2021-09-24T17:38:22.534235Z"
        },
        "id": "GkmKtWEYtjJp"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "data_path = ['/content/gdrive/MyDrive/AI_13/For_modeling.csv']\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.linear_model import ElasticNetCV\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# from sklearnex import patch_sklearn\n",
        "# patch_sklearn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "run_control": {
          "marked": true
        },
        "id": "JOs3LDQStjJr"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "* Import the data. \n",
        "\n",
        "* Also generate approximately 100 equally spaced x data points over the range of 0 to 1. Using these points, calculate the y-data which represents the \"ground truth\" (the real function) from the equation: $y = sin(2\\pi x)$\n",
        "\n",
        "* Plot the sparse data (`x` vs `y`) and the calculated (\"real\") data.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "filepath = os.sep.join(data_path)\n",
        "data = pd.read_csv(filepath)\n",
        "\n",
        "# X_real = np.linspace(0, 1.0, 100)\n",
        "# Y_real = np.sin(2 * np.pi * X_real)"
      ],
      "metadata": {
        "id": "7OlteHB41p81"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:38:25.599001Z",
          "start_time": "2021-09-24T17:38:25.042471Z"
        },
        "id": "49NVDcO1tjJs"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the object (string) columns\n",
        "mask = data.dtypes == object\n",
        "categorical_cols = data.columns[mask]"
      ],
      "metadata": {
        "id": "VS0xW65HvsPL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine how many extra columns would be created\n",
        "num_ohc_cols = (data[categorical_cols]\n",
        "                .apply(lambda x: x.nunique())\n",
        "                .sort_values(ascending=False))\n",
        "\n",
        "\n",
        "# No need to encode if there is only one value\n",
        "small_num_ohc_cols = num_ohc_cols.loc[num_ohc_cols>1]\n",
        "\n",
        "# Number of one-hot columns is one less than the number of categories\n",
        "small_num_ohc_cols -= 1\n",
        "\n",
        "# This is 215 columns, assuming the original ones are dropped. \n",
        "# This is quite a few extra columns!\n",
        "small_num_ohc_cols.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48kI6PyHvoSM",
        "outputId": "7cbaaddf-6295-46cf-ca99-ca81d35a5e31"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy of the data\n",
        "data_ohc = data.copy()\n",
        "\n",
        "# The encoders\n",
        "le = LabelEncoder()\n",
        "ohc = OneHotEncoder()\n",
        "\n",
        "for col in num_ohc_cols.index:\n",
        "    \n",
        "    # Integer encode the string categories\n",
        "    dat = le.fit_transform(data_ohc[col]).astype(int)\n",
        "    \n",
        "    # Remove the original column from the dataframe\n",
        "    data_ohc = data_ohc.drop(col, axis=1)\n",
        "\n",
        "    # One hot encode the data--this returns a sparse array\n",
        "    new_dat = ohc.fit_transform(dat.reshape(-1,1))\n",
        "\n",
        "    # Create unique column names\n",
        "    n_cols = new_dat.shape[1]\n",
        "    col_names = ['_'.join([col, str(x)]) for x in range(n_cols)]\n",
        "\n",
        "    # Create the new dataframe\n",
        "    new_df = pd.DataFrame(new_dat.toarray(), \n",
        "                          index=data_ohc.index, \n",
        "                          columns=col_names)\n",
        "\n",
        "    # Append the new data to the dataframe\n",
        "    data_ohc = pd.concat([data_ohc, new_df], axis=1)\n",
        "\n",
        "print(data_ohc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc68orxQvcwX",
        "outputId": "94a4052e-5436-4ff7-8e2c-47ecc10fab04"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Unnamed: 0  Duration  Distance      PLong       PLatd      DLong  \\\n",
            "0                 0         3        50  37.544666  126.888359  37.544666   \n",
            "1                 1        24      7670  37.506199  127.003944  37.551250   \n",
            "2                 2         8      1390  37.544590  127.057083  37.537014   \n",
            "3                 3         8      1820  37.571102  127.023560  37.561447   \n",
            "4                 4         4       850  37.573242  127.015907  37.565849   \n",
            "...             ...       ...       ...        ...         ...        ...   \n",
            "9601134     9830306        67      6930  37.562607  127.051308  37.562607   \n",
            "9601135     9830307        58      8320  37.511982  127.085052  37.476028   \n",
            "9601136     9830308       118      2730  37.506199  127.003944  37.514870   \n",
            "9601137     9830309        90     17170  37.489750  126.927467  37.565903   \n",
            "9601138     9830310       116      1410  37.585655  127.075050  37.585655   \n",
            "\n",
            "              DLatd  Haversine  Pmonth  Pday  ...  Dmin  DDweek  Temp  Precip  \\\n",
            "0        126.888359   0.000000       1     1  ...     4       0  -3.2     0.0   \n",
            "1        127.035103   5.713529       1     1  ...    25       0  -3.2     0.0   \n",
            "2        127.061096   0.913702       1     1  ...     9       0  -3.2     0.0   \n",
            "3        127.034920   1.468027       1     1  ...    10       0  -3.2     0.0   \n",
            "4        127.016403   0.823227       1     1  ...     6       0  -3.2     0.0   \n",
            "...             ...        ...     ...   ...  ...   ...     ...   ...     ...   \n",
            "9601134  127.051308   0.000000      12    31  ...    51       1  -5.2     0.0   \n",
            "9601135  127.105942   4.402263      12    31  ...    53       1  -5.2     0.0   \n",
            "9601136  127.015282   1.389156      12    31  ...    58       1  -5.4     0.0   \n",
            "9601137  126.901184   8.779307      12    31  ...    26       1  -5.2     0.0   \n",
            "9601138  127.075050   0.000000      12    31  ...    30       1  -5.2     0.0   \n",
            "\n",
            "         Wind  Humid  Solar  Snow  GroundTemp  Dust  \n",
            "0         0.5   40.0    0.0   0.0        -2.2  25.0  \n",
            "1         0.5   40.0    0.0   0.0        -2.2  25.0  \n",
            "2         0.5   40.0    0.0   0.0        -2.2  25.0  \n",
            "3         0.5   40.0    0.0   0.0        -2.2  25.0  \n",
            "4         0.5   40.0    0.0   0.0        -2.2  25.0  \n",
            "...       ...    ...    ...   ...         ...   ...  \n",
            "9601134   1.6   47.0    0.0   0.0        -5.1  53.0  \n",
            "9601135   1.6   47.0    0.0   0.0        -5.1  53.0  \n",
            "9601136   1.3   46.0    0.0   0.0        -5.0  49.0  \n",
            "9601137   1.6   47.0    0.0   0.0        -5.1  53.0  \n",
            "9601138   1.6   47.0    0.0   0.0        -5.1  53.0  \n",
            "\n",
            "[9601139 rows x 26 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y_col = 'age'\n",
        "\n",
        "# # Split the data that is not one-hot encoded\n",
        "# feature_cols = [x for x in data.columns if x != y_col]\n",
        "# X_data = data[feature_cols]\n",
        "# y_data = data[y_col]\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, \n",
        "#                                                     test_size=0.3, random_state=42)\n",
        "# # Split the data that is one-hot encoded\n",
        "# feature_cols = [x for x in data_ohc.columns if x != y_col]\n",
        "# X_data_ohc = data_ohc[feature_cols]\n",
        "# y_data_ohc = data_ohc[y_col]\n",
        "\n",
        "# X_train_ohc, X_test_ohc, y_train_ohc, y_test_ohc = train_test_split(X_data_ohc, y_data_ohc, \n",
        "#                                                     test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "eoBftYGVvKVJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm9NMaAstjJs"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "\n",
        "* Using the `PolynomialFeatures` class from Scikit-learn's preprocessing library, create 20th order polynomial features.\n",
        "* Fit this data using linear regression. \n",
        "* Plot the resulting predicted value compared to the calculated data.\n",
        "\n",
        "Note that `PolynomialFeatures` requires either a dataframe (with one column, not a Series) or a 2D array of dimension (`X`, 1), where `X` is the length."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.shape[1])\n",
        "\n",
        "# Remove the string columns from the dataframe\n",
        "data = data.drop(num_ohc_cols.index, axis=1)\n",
        "\n",
        "print(data.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVYD_fVtyU_B",
        "outputId": "b363f935-72e2-4e81-cfbf-e510ca3d5533"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_col = 'Duration'\n",
        "\n",
        "# Split the data that is not one-hot encoded\n",
        "feature_cols = [x for x in data.columns if x != y_col]\n",
        "X_data = data[feature_cols]\n",
        "y_data = data[y_col]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, \n",
        "                                                    test_size=0.3, random_state=42)\n",
        "# Split the data that is one-hot encoded\n",
        "feature_cols = [x for x in data_ohc.columns if x != y_col]\n",
        "X_data_ohc = data_ohc[feature_cols]\n",
        "y_data_ohc = data_ohc[y_col]\n",
        "\n",
        "X_train_ohc, X_test_ohc, y_train_ohc, y_test_ohc = train_test_split(X_data_ohc, y_data_ohc, \n",
        "                                                    test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "uh2bUuckynQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR = LinearRegression()\n",
        "\n",
        "# Storage for error values\n",
        "error_df = list()\n",
        "\n",
        "# Data that have not been one-hot encoded\n",
        "LR = LR.fit(X_train, y_train)\n",
        "y_train_pred = LR.predict(X_train)\n",
        "y_test_pred = LR.predict(X_test)\n",
        "\n",
        "error_df.append(pd.Series({'train': mean_squared_error(y_train, y_train_pred),\n",
        "                           'test' : mean_squared_error(y_test,  y_test_pred)},\n",
        "                           name='no enc'))\n",
        "\n",
        "# Data that have been one-hot encoded\n",
        "LR = LR.fit(X_train_ohc, y_train_ohc)\n",
        "y_train_ohc_pred = LR.predict(X_train_ohc)\n",
        "y_test_ohc_pred = LR.predict(X_test_ohc)\n",
        "\n",
        "error_df.append(pd.Series({'train': mean_squared_error(y_train_ohc, y_train_ohc_pred),\n",
        "                           'test' : mean_squared_error(y_test_ohc,  y_test_ohc_pred)},\n",
        "                          name='one-hot enc'))\n",
        "\n",
        "# Assemble the results\n",
        "error_df = pd.concat(error_df, axis=1)\n",
        "error_df"
      ],
      "metadata": {
        "id": "dDMMBw91yvCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scalers = {'standard': StandardScaler(),\n",
        "           'minmax': MinMaxScaler(),\n",
        "           'maxabs': MaxAbsScaler()}\n",
        "\n",
        "training_test_sets = {\n",
        "    'not_encoded': (X_train, y_train, X_test, y_test),\n",
        "    'one_hot_encoded': (X_train_ohc, y_train_ohc, X_test_ohc, y_test_ohc)}\n",
        "\n",
        "\n",
        "# Get the list of float columns, and the float data\n",
        "# so that we don't scale something we already scaled. \n",
        "# We're supposed to scale the original data each time\n",
        "mask = X_train.dtypes == float\n",
        "float_columns = X_train.columns[mask]\n",
        "\n",
        "# initialize model\n",
        "LR = LinearRegression()\n",
        "\n",
        "# iterate over all possible combinations and get the errors\n",
        "errors = {}\n",
        "for encoding_label, (_X_train, _y_train, _X_test, _y_test) in training_test_sets.items():\n",
        "    for scaler_label, scaler in scalers.items():\n",
        "        trainingset = _X_train.copy()  # copy because we dont want to scale this more than once.\n",
        "        testset = _X_test.copy()\n",
        "        trainingset[float_columns] = scaler.fit_transform(trainingset[float_columns])\n",
        "        testset[float_columns] = scaler.transform(testset[float_columns])\n",
        "        LR.fit(trainingset, _y_train)\n",
        "        predictions = LR.predict(testset)\n",
        "        key = encoding_label + ' - ' + scaler_label + 'scaling'\n",
        "        errors[key] = mean_squared_error(_y_test, predictions)\n",
        "\n",
        "errors = pd.Series(errors)\n",
        "print(errors.to_string())\n",
        "print('-' * 80)\n",
        "for key, error_val in errors.items():\n",
        "    print(key, error_val)"
      ],
      "metadata": {
        "id": "AcazPNV00wxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F94O9GQItjJt"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "* Perform the regression on using the data with polynomial features using ridge regression ($\\alpha$=0.001) and lasso regression ($\\alpha$=0.0001). \n",
        "* Plot the results, as was done in Question 1. \n",
        "* Also plot the magnitude of the coefficients obtained from these regressions, and compare them to those obtained from linear regression in the previous question. The linear regression coefficients will likely need a separate plot (or their own y-axis) due to their large magnitude. \n",
        "\n",
        "What does the comparatively large magnitude of the data tell you about the role of regularization?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mute the sklearn warning about regularization\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', module='sklearn')\n",
        "\n",
        "# The ridge regression model\n",
        "rr = Ridge(alpha=0.001)\n",
        "rr = rr.fit(X_poly, y_data)\n",
        "Y_pred_rr = rr.predict(X_poly)\n",
        "\n",
        "# The lasso regression model\n",
        "lassor = Lasso(alpha=0.0001)\n",
        "lassor = lassor.fit(X_poly, Y_data)\n",
        "Y_pred_lr = lassor.predict(X_poly)\n",
        "\n",
        "# The plot of the predicted values\n",
        "plt.plot(X_data, Y_data, marker='o', ls='', label='data')\n",
        "plt.plot(X_real, Y_real, ls='--', label='real function')\n",
        "plt.plot(X_data, Y_pred, label='linear regression', marker='^', alpha=.5)\n",
        "plt.plot(X_data, Y_pred_rr, label='ridge regression', marker='^', alpha=.5)\n",
        "plt.plot(X_data, Y_pred_lr, label='lasso regression', marker='^', alpha=.5)\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.set(xlabel='x data', ylabel='y data')"
      ],
      "metadata": {
        "id": "EvtHk_Y40UvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:38:27.959448Z",
          "start_time": "2021-09-24T17:38:27.284316Z"
        },
        "id": "STLfuSzstjJu"
      },
      "outputs": [],
      "source": [
        "# Mute the sklearn warning about regularization\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', module='sklearn')\n",
        "\n",
        "# The ridge regression model\n",
        "rr = Ridge(alpha=0.001)\n",
        "rr = rr.fit(X_poly, Y_data)\n",
        "Y_pred_rr = rr.predict(X_poly)\n",
        "\n",
        "# The lasso regression model\n",
        "lassor = Lasso(alpha=0.0001)\n",
        "lassor = lassor.fit(X_poly, Y_data)\n",
        "Y_pred_lr = lassor.predict(X_poly)\n",
        "\n",
        "# The plot of the predicted values\n",
        "plt.plot(X_data, Y_data, marker='o', ls='', label='data')\n",
        "plt.plot(X_real, Y_real, ls='--', label='real function')\n",
        "plt.plot(X_data, Y_pred, label='linear regression', marker='^', alpha=.5)\n",
        "plt.plot(X_data, Y_pred_rr, label='ridge regression', marker='^', alpha=.5)\n",
        "plt.plot(X_data, Y_pred_lr, label='lasso regression', marker='^', alpha=.5)\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.set(xlabel='x data', ylabel='y data');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:38:28.007446Z",
          "start_time": "2021-09-24T17:38:27.962447Z"
        },
        "id": "MNarSSCrtjJu"
      },
      "outputs": [],
      "source": [
        "# let's look at the absolute value of coefficients for each model\n",
        "\n",
        "coefficients = pd.DataFrame()\n",
        "coefficients['linear regression'] = lr.coef_.ravel()\n",
        "coefficients['ridge regression'] = rr.coef_.ravel()\n",
        "coefficients['lasso regression'] = lassor.coef_.ravel()\n",
        "coefficients = coefficients.applymap(abs)\n",
        "\n",
        "coefficients.describe()  # Huge difference in scale between non-regularized vs regularized regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:38:28.997515Z",
          "start_time": "2021-09-24T17:38:28.012451Z"
        },
        "id": "NLiACOLctjJv"
      },
      "outputs": [],
      "source": [
        "colors = sns.color_palette()\n",
        "\n",
        "# Setup the dual y-axes\n",
        "ax1 = plt.axes()\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "# Plot the linear regression data\n",
        "ax1.plot(lr.coef_.ravel(), \n",
        "         color=colors[0], marker='o', label='linear regression')\n",
        "\n",
        "# Plot the regularization data sets\n",
        "ax2.plot(rr.coef_.ravel(), \n",
        "         color=colors[1], marker='o', label='ridge regression')\n",
        "\n",
        "ax2.plot(lassor.coef_.ravel(), \n",
        "         color=colors[2], marker='o', label='lasso regression')\n",
        "\n",
        "# Customize axes scales\n",
        "ax1.set_ylim(-2e14, 2e14)\n",
        "ax2.set_ylim(-25, 25)\n",
        "\n",
        "# Combine the legends\n",
        "h1, l1 = ax1.get_legend_handles_labels()\n",
        "h2, l2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(h1+h2, l1+l2)\n",
        "\n",
        "ax1.set(xlabel='coefficients',ylabel='linear regression')\n",
        "ax2.set(ylabel='ridge and lasso regression')\n",
        "\n",
        "ax1.set_xticks(range(len(lr.coef_)));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1MDklPftjJv"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "For the remaining questions, we will be working with the [data set](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) from last lesson, which is based on housing prices in Ames, Iowa. There are an extensive number of features--see the exercises from week three for a discussion of these features.\n",
        "\n",
        "To begin:\n",
        "\n",
        "* Import the data with Pandas, remove any null values, and one hot encode categoricals. Either Scikit-learn's feature encoders or Pandas `get_dummies` method can be used.\n",
        "* Split the data into train and test sets. \n",
        "* Log transform skewed features. \n",
        "* Scaling can be attempted, although it can be interesting to see how well regularization works without scaling features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:38:29.060087Z",
          "start_time": "2021-09-24T17:38:29.001514Z"
        },
        "id": "P0bHUf3ztjJv"
      },
      "outputs": [],
      "source": [
        "filepath = os.sep.join(data_path + ['Ames_Housing_Sales.csv'])\n",
        "data = pd.read_csv(filepath, sep=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-03-10T02:40:49.956043Z",
          "start_time": "2017-03-09T21:40:49.950878-05:00"
        },
        "id": "MfMmLjmMtjJw"
      },
      "source": [
        "Create a list of categorial data and one-hot encode. Pandas one-hot encoder (`get_dummies`) works well with data that is defined as a categorical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:38:29.215090Z",
          "start_time": "2021-09-24T17:38:29.064089Z"
        },
        "id": "HR04gA1ntjJw"
      },
      "outputs": [],
      "source": [
        "# Get a Pd.Series consisting of all the string categoricals\n",
        "one_hot_encode_cols = data.dtypes[data.dtypes == object]  # filtering by string categoricals\n",
        "one_hot_encode_cols = one_hot_encode_cols.index.tolist()  # list of categorical fields\n",
        "\n",
        "# Here we see another way of one-hot-encoding:\n",
        "# Encode these columns as categoricals so one hot encoding works on split data (if desired)\n",
        "for col in one_hot_encode_cols:\n",
        "    data[col] = pd.Categorical(data[col])\n",
        "\n",
        "# Do the one hot encoding\n",
        "data = pd.get_dummies(data, columns=one_hot_encode_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMyw4u8AtjJx"
      },
      "source": [
        "Next, split the data in train and test data sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:38:29.247089Z",
          "start_time": "2021-09-24T17:38:29.218091Z"
        },
        "id": "iPY_D1pgtjJx"
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(data, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrfbQPAotjJx"
      },
      "source": [
        "There are a number of columns that have skewed features--a log transformation can be applied to them. Note that this includes the `SalePrice`, our predictor. However, let's keep that one as is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:38:29.262091Z",
          "start_time": "2021-09-24T17:38:29.250089Z"
        },
        "id": "Yf28A4-xtjJy"
      },
      "outputs": [],
      "source": [
        "# Create a list of float colums to check for skewing\n",
        "mask = data.dtypes == float\n",
        "float_cols = data.columns[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:38:29.309089Z",
          "start_time": "2021-09-24T17:38:29.265093Z"
        },
        "id": "IDVQ0-DdtjJy"
      },
      "outputs": [],
      "source": [
        "skew_limit = 0.75\n",
        "skew_vals = train[float_cols].skew()\n",
        "\n",
        "skew_cols = (skew_vals\n",
        "             .sort_values(ascending=False)\n",
        "             .to_frame()\n",
        "             .rename(columns={0:'Skew'})\n",
        "             .query('abs(Skew) > {0}'.format(skew_limit)))\n",
        "\n",
        "skew_cols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzS5fVAdtjJz"
      },
      "source": [
        "Transform all the columns where the skew is greater than 0.75, excluding \"SalePrice\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:38:30.393629Z",
          "start_time": "2021-09-24T17:38:29.312092Z"
        },
        "id": "2hU0D9cytjJz"
      },
      "outputs": [],
      "source": [
        "# OPTIONAL: Let's look at what happens to one of these features, when we apply np.log1p visually.\n",
        "\n",
        "field = \"BsmtFinSF1\"\n",
        "fig, (ax_before, ax_after) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "train[field].hist(ax=ax_before)\n",
        "train[field].apply(np.log1p).hist(ax=ax_after)\n",
        "ax_before.set(title='before np.log1p', ylabel='frequency', xlabel='value')\n",
        "ax_after.set(title='after np.log1p', ylabel='frequency', xlabel='value')\n",
        "fig.suptitle('Field \"{}\"'.format(field));\n",
        "# a little bit better"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:38:30.425240Z",
          "start_time": "2021-09-24T17:38:30.397628Z"
        },
        "id": "8j8J-LRVtjJz"
      },
      "outputs": [],
      "source": [
        "# Mute the setting wtih a copy warnings\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "for col in skew_cols.index.tolist():\n",
        "    if col == \"SalePrice\":\n",
        "        continue\n",
        "    train[col] = np.log1p(train[col])\n",
        "    test[col]  = test[col].apply(np.log1p)  # same thing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q52s-6fVtjJ0"
      },
      "source": [
        "Separate features from predictor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:38:30.455238Z",
          "start_time": "2021-09-24T17:38:30.428240Z"
        },
        "id": "7T441wwztjJ0"
      },
      "outputs": [],
      "source": [
        "feature_cols = [x for x in train.columns if x != 'SalePrice']\n",
        "X_train = train[feature_cols].to_numpy()\n",
        "y_train = train['SalePrice'].to_numpy()\n",
        "\n",
        "X_test  = test[feature_cols].to_numpy()\n",
        "y_test  = test['SalePrice'].to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-02-21T09:11:03.256453",
          "start_time": "2017-02-21T09:11:03.241117"
        },
        "id": "TUBABevntjJ0"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "* Write a function **`rmse`** that takes in truth and prediction values and returns the root-mean-squared error. Use sklearn's `mean_squared_error`.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:38:30.471241Z",
          "start_time": "2021-09-24T17:38:30.458242Z"
        },
        "id": "BeNnoyWetjJ0"
      },
      "outputs": [],
      "source": [
        "def rmse(ytrue, ypredicted):\n",
        "    return np.sqrt(mean_squared_error(ytrue, ypredicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN0863_TtjJ1"
      },
      "source": [
        "* Fit a basic linear regression model\n",
        "* print the root-mean-squared error for this model\n",
        "* plot the predicted vs actual sale price based on the model.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:38:30.534242Z",
          "start_time": "2021-09-24T17:38:30.474241Z"
        },
        "id": "WQwjc-bBtjJ1"
      },
      "outputs": [],
      "source": [
        "linearRegression = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "linearRegression_rmse = rmse(y_test, linearRegression.predict(X_test))\n",
        "\n",
        "print(linearRegression_rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:38:31.163275Z",
          "start_time": "2021-09-24T17:38:30.537244Z"
        },
        "id": "CMSAI7u8tjJ2"
      },
      "outputs": [],
      "source": [
        "f = plt.figure(figsize=(6,6))\n",
        "ax = plt.axes()\n",
        "\n",
        "ax.plot(y_test, linearRegression.predict(X_test), \n",
        "         marker='o', ls='', ms=3.0)\n",
        "\n",
        "lim = (0, y_test.max())\n",
        "\n",
        "ax.set(xlabel='Actual Price', \n",
        "       ylabel='Predicted Price', \n",
        "       xlim=lim,\n",
        "       ylim=lim,\n",
        "       title='Linear Regression Results');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaqPTVTPtjJ2"
      },
      "source": [
        "## Question 6\n",
        "\n",
        "Ridge regression uses L2 normalization to reduce the magnitude of the coefficients. This can be helpful in situations where there is high variance. The regularization functions in Scikit-learn each contain versions that have cross-validation built in.\n",
        "\n",
        "* Fit a regular (non-cross validated) Ridge model to a range of $\\alpha$ values and plot the RMSE using the cross validated error function you created above.\n",
        "* Use $$[0.005, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 80]$$ as the range of alphas.\n",
        "* Then repeat the fitting of the Ridge models using the range of $\\alpha$ values from the prior section. Compare the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d3xef7itjJ2"
      },
      "source": [
        "Now for the `RidgeCV` method. It's not possible to get the alpha values for the models that weren't selected, unfortunately. The resulting error values and $\\alpha$ values are very similar to those obtained above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:38:31.711283Z",
          "start_time": "2021-09-24T17:38:31.166278Z"
        },
        "id": "Z6xiKcvHtjJ3"
      },
      "outputs": [],
      "source": [
        "alphas = [0.005, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 80]\n",
        "\n",
        "ridgeCV = RidgeCV(alphas=alphas, \n",
        "                  cv=4).fit(X_train, y_train)\n",
        "\n",
        "ridgeCV_rmse = rmse(y_test, ridgeCV.predict(X_test))\n",
        "\n",
        "print(ridgeCV.alpha_, ridgeCV_rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQd0i7Y8tjJ3"
      },
      "source": [
        "## Question 7\n",
        "\n",
        "\n",
        "Much like the `RidgeCV` function, there is also a `LassoCV` function that uses an L1 regularization function and cross-validation. L1 regularization will selectively shrink some coefficients, effectively performing feature elimination.\n",
        "\n",
        "The `LassoCV` function does not allow the scoring function to be set. However, the custom error function (`rmse`) created above can be used to evaluate the error on the final model.\n",
        "\n",
        "Similarly, there is also an elastic net function with cross validation, `ElasticNetCV`, which is a combination of L2 and L1 regularization.\n",
        "\n",
        "* Fit a Lasso model using cross validation and determine the optimum value for $\\alpha$ and the RMSE using the function created above. Note that the magnitude of $\\alpha$ may be different from the Ridge model.\n",
        "* Repeat this with the Elastic net model.\n",
        "* Compare the results via table and/or plot.\n",
        "\n",
        "Use the following alphas:  \n",
        "`[1e-5, 5e-5, 0.0001, 0.0005]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:41:04.384991Z",
          "start_time": "2021-09-24T17:38:31.714283Z"
        },
        "id": "By19r9MhtjJ4"
      },
      "outputs": [],
      "source": [
        "alphas2 = np.array([1e-5, 5e-5, 0.0001, 0.0005])\n",
        "\n",
        "lassoCV = LassoCV(alphas=alphas2,\n",
        "                  max_iter=5e4,\n",
        "                  cv=3).fit(X_train, y_train)\n",
        "\n",
        "lassoCV_rmse = rmse(y_test, lassoCV.predict(X_test))\n",
        "\n",
        "print(lassoCV.alpha_, lassoCV_rmse)  # Lasso is slower"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLExFSWatjJ4"
      },
      "source": [
        "We can determine how many of these features remain non-zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:41:04.400513Z",
          "start_time": "2021-09-24T17:41:04.388514Z"
        },
        "id": "UE7FiLD-tjJ4"
      },
      "outputs": [],
      "source": [
        "print('Of {} coefficients, {} are non-zero with Lasso.'.format(len(lassoCV.coef_), \n",
        "                                                               len(lassoCV.coef_.nonzero()[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2017-02-16T12:03:06.013488",
          "start_time": "2017-02-16T12:03:06.007159"
        },
        "collapsed": true,
        "run_control": {
          "marked": true
        },
        "id": "mJmuOOkstjJ5"
      },
      "source": [
        "Now try the elastic net, with the same alphas as in Lasso, and l1_ratios between 0.1 and 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:42:59.975548Z",
          "start_time": "2021-09-24T17:41:04.407035Z"
        },
        "scrolled": true,
        "id": "rzt6vCg_tjJ5"
      },
      "outputs": [],
      "source": [
        "l1_ratios = np.linspace(0.1, 0.9, 9)\n",
        "\n",
        "elasticNetCV = ElasticNetCV(alphas=alphas2, \n",
        "                            l1_ratio=l1_ratios,\n",
        "                            max_iter=1e4).fit(X_train, y_train)\n",
        "elasticNetCV_rmse = rmse(y_test, elasticNetCV.predict(X_test))\n",
        "\n",
        "print(elasticNetCV.alpha_, elasticNetCV.l1_ratio_, elasticNetCV_rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9K5-GWMtjJ5"
      },
      "source": [
        "Comparing the RMSE calculation from all models is easiest in a table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:42:59.992496Z",
          "start_time": "2021-09-24T17:42:59.976916Z"
        },
        "id": "qyCYIj8vtjJ5"
      },
      "outputs": [],
      "source": [
        "rmse_vals = [linearRegression_rmse, ridgeCV_rmse, lassoCV_rmse, elasticNetCV_rmse]\n",
        "\n",
        "labels = ['Linear', 'Ridge', 'Lasso', 'ElasticNet']\n",
        "\n",
        "rmse_df = pd.Series(rmse_vals, index=labels).to_frame()\n",
        "rmse_df.rename(columns={0: 'RMSE'}, inplace=1)\n",
        "rmse_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRfYTWy5tjJ6"
      },
      "source": [
        "We can also make a plot of actual vs predicted housing prices as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:43:00.445092Z",
          "start_time": "2021-09-24T17:42:59.994556Z"
        },
        "id": "eyo06k66tjJ6"
      },
      "outputs": [],
      "source": [
        "f = plt.figure(figsize=(6,6))\n",
        "ax = plt.axes()\n",
        "\n",
        "labels = ['Ridge', 'Lasso', 'ElasticNet']\n",
        "\n",
        "models = [ridgeCV, lassoCV, elasticNetCV]\n",
        "\n",
        "for mod, lab in zip(models, labels):\n",
        "    ax.plot(y_test, mod.predict(X_test), \n",
        "             marker='o', ls='', ms=3.0, label=lab)\n",
        "\n",
        "\n",
        "leg = plt.legend(frameon=True)\n",
        "leg.get_frame().set_edgecolor('black')\n",
        "leg.get_frame().set_linewidth(1.0)\n",
        "\n",
        "ax.set(xlabel='Actual Price', \n",
        "       ylabel='Predicted Price', \n",
        "       title='Linear Regression Results');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1sgmucHtjJ6"
      },
      "source": [
        "## Question 8\n",
        "\n",
        "Let's explore Stochastic gradient descent in this exercise.  \n",
        "Recall that Linear models in general are sensitive to scaling.\n",
        "However, SGD is *very* sensitive to scaling.  \n",
        "Moreover, a high value of learning rate can cause the algorithm to diverge, whereas a too low value may take too long to converge.\n",
        "\n",
        "* Fit a stochastic gradient descent model without a regularization penalty (the relevant parameter is `penalty`).\n",
        "* Now fit stochastic gradient descent models with each of the three penalties (L2, L1, Elastic Net) using the parameter values determined by cross validation above. \n",
        "* Do not scale the data before fitting the model.  \n",
        "* Compare the results to those obtained without using stochastic gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:43:00.703405Z",
          "start_time": "2021-09-24T17:43:00.447129Z"
        },
        "id": "So4fi2BxtjJ6"
      },
      "outputs": [],
      "source": [
        "# Import SGDRegressor and prepare the parameters\n",
        "model_parameters_dict = {\n",
        "    'Linear': {'penalty': 'none'},\n",
        "    'Lasso': {'penalty': 'l2',\n",
        "           'alpha': lassoCV.alpha_},\n",
        "    'Ridge': {'penalty': 'l1',\n",
        "           'alpha': ridgeCV_rmse},\n",
        "    'ElasticNet': {'penalty': 'elasticnet', \n",
        "                   'alpha': elasticNetCV.alpha_,\n",
        "                   'l1_ratio': elasticNetCV.l1_ratio_}\n",
        "}\n",
        "\n",
        "new_rmses = {}\n",
        "for modellabel, parameters in model_parameters_dict.items():\n",
        "    # following notation passes the dict items as arguments\n",
        "    SGD = SGDRegressor(**parameters)\n",
        "    SGD.fit(X_train, y_train)\n",
        "    new_rmses[modellabel] = rmse(y_test, SGD.predict(X_test))\n",
        "\n",
        "rmse_df['RMSE-SGD'] = pd.Series(new_rmses)\n",
        "rmse_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq7QU9TftjJ7"
      },
      "source": [
        "Notice how high the error values are! The algorithm is diverging. This can be due to scaling and/or learning rate being too high. Let's adjust the learning rate and see what happens.\n",
        "\n",
        "* Pass in `eta0=1e-7` when creating the instance of `SGDClassifier`.\n",
        "* Re-compute the errors for all the penalties and compare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:43:00.929966Z",
          "start_time": "2021-09-24T17:43:00.705372Z"
        },
        "id": "u5xLQ7TytjJ7"
      },
      "outputs": [],
      "source": [
        "# Import SGDRegressor and prepare the parameters\n",
        "model_parameters_dict = {\n",
        "    'Linear': {'penalty': 'none'},\n",
        "    'Lasso': {'penalty': 'l2',\n",
        "           'alpha': lassoCV.alpha_},\n",
        "    'Ridge': {'penalty': 'l1',\n",
        "           'alpha': ridgeCV_rmse},\n",
        "    'ElasticNet': {'penalty': 'elasticnet', \n",
        "                   'alpha': elasticNetCV.alpha_,\n",
        "                   'l1_ratio': elasticNetCV.l1_ratio_}\n",
        "}\n",
        "\n",
        "new_rmses = {}\n",
        "for modellabel, parameters in model_parameters_dict.items():\n",
        "    # following notation passes the dict items as arguments\n",
        "    SGD = SGDRegressor(eta0=1e-7, **parameters)\n",
        "    SGD.fit(X_train, y_train)\n",
        "    new_rmses[modellabel] = rmse(y_test, SGD.predict(X_test))\n",
        "\n",
        "rmse_df['RMSE-SGD-learningrate'] = pd.Series(new_rmses)\n",
        "rmse_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP_mqZ2OtjJ7"
      },
      "source": [
        "Now let's scale our training data and try again.\n",
        "\n",
        "* Fit a `MinMaxScaler` to `X_train` create a variable `X_train_scaled`.\n",
        "* Using the scaler, transform `X_test` and create a variable `X_test_scaled`. \n",
        "* Apply the same versions of SGD to them and compare the results. Don't pass in a eta0 this time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:43:01.479966Z",
          "start_time": "2021-09-24T17:43:00.932018Z"
        },
        "id": "duRbn4MAtjJ7"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "new_rmses = {}\n",
        "for modellabel, parameters in model_parameters_dict.items():\n",
        "    # following notation passes the dict items as arguments\n",
        "    SGD = SGDRegressor(**parameters)\n",
        "    SGD.fit(X_train_scaled, y_train)\n",
        "    new_rmses[modellabel] = rmse(y_test, SGD.predict(X_test_scaled))\n",
        "\n",
        "rmse_df['RMSE-SGD-scaled'] = pd.Series(new_rmses)\n",
        "rmse_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-09-24T17:43:02.067967Z",
          "start_time": "2021-09-24T17:43:01.481961Z"
        },
        "id": "X4mEMnA9tjJ8"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "new_rmses = {}\n",
        "for modellabel, parameters in model_parameters_dict.items():\n",
        "    # following notation passes the dict items as arguments\n",
        "    SGD = SGDRegressor(**parameters)\n",
        "    SGD.fit(X_train_scaled, y_train)\n",
        "    new_rmses[modellabel] = rmse(y_test, SGD.predict(X_test_scaled))\n",
        "\n",
        "rmse_df['RMSE-SGD-scaled'] = pd.Series(new_rmses)\n",
        "rmse_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCuBPYDVtjJ8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}